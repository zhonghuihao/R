# 
# install.packages("Rgraphviz")
# install.packages("SnowballC")
# install.packages("tm")
# install.packages("magrittr")
# install.packages("openNLPdata")

library(tm)
library(magrittr)

## Lording a Corpus
(cname <- file.path("C:/Users/2200499/Documents/R","data","TOP"))
(docs <- Corpus(DirSource(cname),readerControl = list(language = "japanese")))
cont <- read.csv("C:/Users/2200499/Documents/R/data/TOP/ttt.txt",header = F)
# docs <- Corpus(DataframeSource(cont))

class(docs)
class(docs[[1]])
summary(docs)

## Exploring the Corpus
inspect(docs[1])

## Preparing the Corpus
# getTransformations()

library(RMeCab)
aa <- RMeCabText("C:/Users/2200499/Documents/R/data/TOP/ttt.txt")
aa[1:3]

cont$V1
## simple transforms
# stopwd <- read.csv("C:/Users/2200499/Documents/R/lib/Stopwords_jp.csv",sep=",",header = TRUE)
# stopword_ja <- as.character(stopwd[,])
# matchs <- paste(stopword_ja,collapse="|")  
# toSpace <- content_transformer(function(x,pattern) gsub(pattern," ",x))
# docs <- tm_map(docs,toSpace,matchs)
# docs <- tm_map(docs,toSpace,"/|>|@|\\|<br>")

## conversion to lower case 
docs <- tm_map(docs,content_transformer(tolower))

## remove numbers
docs <- tm_map(docs,removeNumbers)

## remove punctuation
docs <- tm_map(docs,removePunctuation)

## remove janpanese stop words
# stopwd <- read.csv("C:/Users/2200499/Documents/R/lib/Stopwords_jp.csv",sep=",",header = F)
# stopword_ja <- as.character(stopwd[,1])
# docs <- tm_map(docs,removeWords,stopword_ja)
# docs <- tm_map(docs,removeWords,stopwords("english"))

## remove own stop words
docs <- tm_map(docs,removeWords,c("お疲れ様です","我々","の","ました","する"))

# strip whitespace
docs <- tm_map(docs ,stripWhitespace)

# specific transformations 
toString <- content_transformer(function(x,from,to)gsub(from,to,x))
docs <- tm_map(docs,toString,"zhonghuihao","zhh")
docs <- tm_map(docs,toString,"ながた","永田")

# stemming
# donot support japannese and chinese 
# library(SnowballC)
# getStemLanguages()
# docs <- tm_map(docs,stemDocument)

# creating a document term matrix
dtm <- DocumentTermMatrix(docs)

class(dtm)
dim(dtm)

(tdm <- TermDocumentMatrix(docs))

# exploring the document term matrix
(freq <- colSums(as.matrix(dtm)))
(ord <- order(freq))
# freq[head(ord)]
# freq[tail(ord)]

# distribution of term frequencies
# head(table(freq),15)
# tail(table(freq),15)

# conversion to matrix and save to csv
# m <- as.matrix(dtm)

# removing sparse terms
dtms <- removeSparseTerms(dtm,0.2)

# identify frequent items and association
findFreqTerms(dtms,lowfreq = 3)
# findFreqTerms(dtms,highfreq=1)
findAssocs(dtms,"small",corlimit = 0.1)

# correlations plots
# library(Rgraphviz)
# plot(dtms,
#      terms=findFreqTerms(dtm,lowfreq = 3),
#      corThreshold = 0.5)

# plotting word frequencies
freq <-  sort(colSums(as.matrix(dtms)),decreasing = TRUE)
wf <- data.frame(word=names(freq),freq=freq)
class(wf) 

library(ggplot2)
subset(wf,freq>1)                                      %>%
    ggplot(aes(word,freq))                              +
    geom_bar(stat="identity")                           +
    theme(axis.text.x=element_text(angle=45,hjust=1))   

# word clouds
library(wordcloud)
set.seed(123)
wordcloud(names(freq),freq,min.freq = 100)

# reducing clutter with max words
set.seed(142)
wordcloud(names(freq),freq, max.words=100)

# adding some colour
wordcloud(names(freq),freq,min.freq = 2,colors=brewer.pal(6,"Accent"))
# display.brewer.all()
# brewer.pal.info 

# varying the scaling
wordcloud(names(freq),freq,min.freq = 2,scale=c(5,.3),colors=brewer.pal(6,"Accent"))

# quantitative analysis of text
words <- dtms                            %>%
    as.matrix                            %>%
    colnames                             %>%
      (function(x) x[nchar(x) < 20])

library("openNLPdata")
library("qdap")
length(words)
head(words,15)
summary(nchar(words))
table(nchar(words))
dist_tab(nchar(words))

# word length counts
data.frame(nletters = nchar(words))                 %>%
    ggplot(aes(x=nletters))                          +
    geom_histogram(binwidth = 1)                     +
    geom_vline(xintercept = mean(nchar(words)),
               colour="green",size=1,alpha=.5)       +
    labs(x="number of letters",y = "number of words")

#letter Frequency

#letter an positon heatmap



